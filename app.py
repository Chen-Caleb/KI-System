import streamlit as st
import os
from dotenv import load_dotenv

# é€‚é…æ–°ç‰ˆ LangChain çš„å¯¼å…¥è·¯å¾„
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# 1. åŠ è½½ç¯å¢ƒå˜é‡ (.env æ–‡ä»¶ä¸­çš„ Key)
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# 2. é¡µé¢é…ç½®
st.set_page_config(page_title="KI-Studienberatung", layout="wide")

# ================= ä¾§è¾¹æ  (Sidebar) =================
with st.sidebar:
    st.image(
        "https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1200px-Python-logo-notext.svg.png",
        width=50)
    st.title("Einstellungen")
    st.markdown("---")

    # å¼•ç”¨è‡ªæ–‡æ¡£ å’Œ çš„å¸¸è§å¾·è¯­é—®é¢˜
    st.subheader("HÃ¤ufig gestellte Fragen")

    questions = [
        "Wie viele Fehlversuche im Grundstudium sind erlaubt?",  #
        "Was passiert bei VersÃ¤umnis einer PrÃ¼fung?",  #
        "Was sind die Unterschiede zwischen KF/AM/DAR/KBI?",  #
        "Welche Anforderungen gelten fÃ¼r das Praxissemester?",  #
        "Wie berechnet sich die Modulnote?",  #
        "Wieviel LP hat die Bachelorarbeit?"  #
    ]

    # åˆ›å»ºå¿«é€Ÿæé—®æŒ‰é’®
    for q in questions:
        if st.button(q):
            st.session_state.temp_input = q


# ================= åå°é€»è¾‘ (Backend) =================
@st.cache_resource
def get_vector_store():
    """
    åŠ è½½ PDF å¹¶å»ºç«‹å‘é‡ç´¢å¼•
    """
    if not api_key:
        return None

    try:
        # ç¡®ä¿ä½ çš„ PDF éƒ½åœ¨ data æ–‡ä»¶å¤¹é‡Œ
        loader = PyPDFDirectoryLoader("data")
        docs = loader.load()

        # åˆ‡åˆ†æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        # å»ºç«‹ç´¢å¼•
        embeddings = OpenAIEmbeddings(api_key=api_key)
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"Fehler beim Laden der Dokumente: {e}")
        return None


# ================= ä¸»ç•Œé¢ (Main UI) =================

st.title("ğŸ“ KI-System fÃ¼r Studierendenfragen")
st.markdown("""
Willkommen! Ich bin Ihr KI-Assistent fÃ¼r Fragen zur **Studien- und PrÃ¼fungsordnung (SPO)** und zum Studiengang **Maschinenbau & Mechatronik**.
""")

# æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
if not api_key:
    st.error("âš ï¸ Kein OpenAI API Key gefunden. Bitte Ã¼berprÃ¼fen Sie die .env Datei.")
    st.stop()

# åˆå§‹åŒ–å‘é‡åº“
with st.spinner("System wird initialisiert... Bitte warten."):
    vector_store = get_vector_store()

if vector_store:
    # å®šä¹‰æ¨¡å‹
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3, api_key=api_key)

    # å¾·è¯­ Prompt æ¨¡æ¿
    prompt = ChatPromptTemplate.from_template("""
    Du bist ein hilfreicher Assistent fÃ¼r die Studienberatung an einer deutschen Hochschule.
    Beantworte die Frage des Studenten basierend auf dem folgenden Kontext (AuszÃ¼ge aus der SPO).

    Regeln:
    1. Antworte **ausschlieÃŸlich auf Deutsch**.
    2. Verwende nur Informationen aus dem Kontext. Wenn die Antwort nicht im Kontext steht, sag: "Dazu finde ich keine Informationen in der SPO."
    3. Sei prÃ¤zise und nenne, wenn mÃ¶glich, die relevanten Paragraphen (Â§) oder Abschnitte.

    <context>
    {context}
    </context>

    Frage des Studenten: {input}
    """)

    # åˆ›å»ºæ£€ç´¢é“¾
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = vector_store.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    # èŠå¤©è®°å½•çŠ¶æ€ç®¡ç†
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # å¤„ç†è¾“å…¥ (æ–‡æœ¬æ¡† æˆ– ä¾§è¾¹æ æŒ‰é’®)
    user_input = st.chat_input("Stellen Sie Ihre Frage hier...")

    if "temp_input" in st.session_state and st.session_state.temp_input:
        user_input = st.session_state.temp_input
        st.session_state.temp_input = None

    if user_input:
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # ç”Ÿæˆå›ç­”
        with st.chat_message("assistant"):
            with st.spinner("Suche in der SPO..."):
                try:
                    response = retrieval_chain.invoke({"input": user_input})
                    answer = response['answer']

                    st.markdown(answer)

                    # æ˜¾ç¤ºæ¥æº (Quellen)
                    with st.expander("Quellen anzeigen (Referenz)"):
                        for i, doc in enumerate(response["context"]):
                            source_page = doc.metadata.get('page', 'Unbekannt')
                            source_file = doc.metadata.get('source', 'Dokument').split('/')[-1]
                            st.markdown(f"**Quelle {i + 1}:** {source_file} (Seite {source_page})")
                            st.caption(doc.page_content[:200] + "...")

                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"Ein Fehler ist aufgetreten: {e}")

else:
    st.error("Datenbank konnte nicht geladen werden.")